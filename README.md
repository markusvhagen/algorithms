# algorithms
Trying to learn different algos

## Big-Oh notation
A way to express how an algorithm scales, also known as time complexity. What is the difference in how much time an algorithm use to decide an output between 10 elements and 1000 elements?

Notation | Name | Description
--- | --- | ---
**O(1)** | constant | The amount of time the algorithm uses is independent of the input - it stays the same all the time.
| **O(log n)** | logarithmic | For each iteration, the amount of data is the halved. For example, if the amount of elements you start with is 100, the next iteration only handle 50, and the next 25, and so on. 
