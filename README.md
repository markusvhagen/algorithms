# algorithms
Trying to learn different algos

## Big-Oh notation
A way to express how an algorithm scales, also known as time complexity. What is the difference in how much time an algorithm use to decide an output between 10 elements and 1000 elements?

Notation | Name | Description
--- | --- | ---
*O(1)* | constant | The amount of time the algorithm uses is independent of the input - it stays the same all the time.
